{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Pre-splits the dataset into training and testing folds for all the classifiers to use.\n",
    "def splitting_dataset(folder_name, dataset_name, num_folds):\n",
    "    from numpy import genfromtxt\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    # Loading the dataset\n",
    "    dataset = genfromtxt(folder_name + '/' + dataset_name, delimiter=\",\")\n",
    "\n",
    "    # Split the dataset into the data and the labels\n",
    "    X = dataset[:, 0 : len(dataset[0]) - 1]\n",
    "    y = dataset[:, len(dataset[0]) - 1]\n",
    "\n",
    "    print(\"Splitting the whole dataset into training and testing folds...\")\n",
    "\n",
    "    # Creating folds with StratifiedKFold.\n",
    "    skf = StratifiedKFold(n_splits = num_folds, random_state = None, shuffle = True)\n",
    "    skf.get_n_splits(X, y)\n",
    "    # Opening the dataset file for copying lines.\n",
    "    f_ds = open(folder_name + \"/\" + dataset_name)\n",
    "    # Creating a list from its lines.\n",
    "    dataset_lines = []\n",
    "    for line in f_ds:\n",
    "        dataset_lines.append(line)\n",
    "\n",
    "    ctr = 0; # For naming the files\n",
    "\n",
    "    # For each fold...\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        train_name = folder_name + '/' + 'fold' + str(ctr) + '_train.data'; # file for train instances\n",
    "        test_name = folder_name + '/' + '/fold' + str(ctr) + '_test.data'; # file for test instances\n",
    "        f_train = open(train_name, 'w')\n",
    "        f_test = open(test_name, 'w')\n",
    "\n",
    "        # Selecting the training and testing instances + labels.\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Printing the training data (+labels) into the train file.\n",
    "        for i in train_index:\n",
    "            f_train.write(dataset_lines[i])\n",
    "\n",
    "        # Printing the testing data (+labels) into the test file.\n",
    "        for i in test_index:\n",
    "            f_test.write(dataset_lines[i])\n",
    "\n",
    "        ctr += 1;\n",
    "        f_train.close()\n",
    "        f_test.close()\n",
    "\n",
    "        print(\"%.2f %%\" % (ctr / num_folds * 100))\n",
    "\n",
    "    f_ds.close()\n",
    "    print(\"The whole dataset has been split into folds!\")    \n",
    "    return\n",
    "\n",
    "\n",
    "# Performs the grid search for all of the classifiers, including the extended ones. \n",
    "def general_grid_search(folder_name, num_folds, missing):\n",
    "    # Grid search and extension for XGBoost.\n",
    "    parameters_xgboost = grid_search_xgboost(folder_name, num_folds, missing)\n",
    "    # Grid search and extension for RF.\n",
    "    parameters_rf = grid_search_rf(folder_name, num_folds, missing)\n",
    "    # Grid search and extension for GB\n",
    "    parameters_gb = grid_search_gb(folder_name, num_folds, missing)\n",
    "    \n",
    "    return parameters_xgboost, parameters_rf, parameters_gb\n",
    "\n",
    "\n",
    "# Grid search for XGBoost. Checks also if the extension of the grid search\n",
    "# is necessary and if it is, performs it by calling the extended grid search function.\n",
    "def grid_search_xgboost(folder_name, num_folds, missing):\n",
    "    from numpy import genfromtxt\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    \n",
    "    # Initializes the parameters' list.\n",
    "    print(\"Initializing the parameters to test for XGBoost...\") \n",
    "    learning_rate = [0.05, 0.1, 0.2]\n",
    "    max_depth = [3, 5, 6, 8]\n",
    "    subsample = [0.5, 0.8, 1]\n",
    "    gamma = [0, 0.1, 0.2, 0.3]\n",
    "    min_child_weight = [1, 3, 5]\n",
    "    print(\"The parameters to test for XGBoost have been initialized!\\n\")\n",
    "    \n",
    "    # Initialize the arrays that'll contain the best parameters for each fold (10). Will be returned at the end.\n",
    "    best_learning_rate = []\n",
    "    best_max_depth = []\n",
    "    best_subsample = []\n",
    "    best_gamma = []\n",
    "    best_min_child_weight = []\n",
    "    \n",
    "    imputer = Imputer(missing_values = missing)    \n",
    "    xgb_model = XGBClassifier(n_estimators = 200)\n",
    "    \n",
    "    # For each fold, loads the training dataset in and perform the grid search on it.\n",
    "    for i in range(0, num_folds):\n",
    "        # Loads the dataset (training fold).\n",
    "        print(\"Loading training dataset...\") \n",
    "        dataset = genfromtxt(folder_name + '/' + 'fold' + str(i) + '_train.data', delimiter=\",\")\n",
    "        print(\"Training dataset was loaded in!\")\n",
    "\n",
    "        # Splits the dataset into the data and the labels.\n",
    "        print(\"Splitting the dataset into data and labels...\")\n",
    "        X = dataset[:, 0 : len(dataset[0]) - 1]\n",
    "        Y = dataset[:, len(dataset[0]) - 1]\n",
    "        X = imputer.fit_transform(X, Y)\n",
    "        print(\"The data and labels from the dataset have been split!\")\n",
    "\n",
    "        # Timed grid search.\n",
    "        start = timer()\n",
    "        param_grid = dict(learning_rate = learning_rate, max_depth = max_depth, \n",
    "                          subsample = subsample, gamma = gamma, min_child_weight = min_child_weight)\n",
    "        print(\"Starting the grid search...\")\n",
    "        kfold = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = 7)\n",
    "        grid_search = GridSearchCV(xgb_model, param_grid, scoring = \"neg_log_loss\", n_jobs = -1, cv = kfold, verbose = 1)\n",
    "        grid_result = grid_search.fit(X, Y)\n",
    "        print(\"The grid search is over!\")\n",
    "        end = timer()\n",
    "        time_grid_xgb.append(end - start)        \n",
    "\n",
    "        # Summarizes results. \n",
    "        print(\"Best: %f using %s \\n\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "        best_learning_rate.append(grid_result.best_params_['learning_rate'])\n",
    "        best_max_depth.append(grid_result.best_params_['max_depth'])\n",
    "        best_subsample.append(grid_result.best_params_['subsample'])\n",
    "        best_gamma.append(grid_result.best_params_['gamma'])\n",
    "        best_min_child_weight.append(grid_result.best_params_['min_child_weight'])\n",
    "        \n",
    "        print(\"%.2f %%\\n\" % ((i + 1) / num_folds * 100))\n",
    "        \n",
    "    print(\"Done with the grid search!\\n\")\n",
    "    \n",
    "    # Grid extension.\n",
    "    parameters_xgboost = grid_extension_xgboost(folder_name, num_folds, missing, best_learning_rate, \n",
    "                                                best_max_depth, best_subsample, best_gamma, best_min_child_weight)\n",
    "    \n",
    "    return parameters_xgboost\n",
    "\n",
    "\n",
    "# Grid search for Random Forest. Checks also if the extension of the grid search\n",
    "# is necessary and if it is, performs it by calling the extended grid search function.\n",
    "def grid_search_rf(folder_name, num_folds, missing):\n",
    "    from numpy import genfromtxt\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    \n",
    "    # Initializes the parameters' list.\n",
    "    print(\"Initializing the parameters to test for Random Forest...\") \n",
    "    max_features = [\"auto\", \"log2\", None]\n",
    "    min_samples_leaf = [1, 25, 50, 70]\n",
    "    max_depth = [None, 5, 8, 10]\n",
    "    min_samples_split = [2, 5, 8, 10]\n",
    "    print(\"The parameters to test for Random Forest have been initialized!\\n\")\n",
    "    \n",
    "    # Initialize the arrays that'll contain the best parameters for each fold (10). Will be returned at the end.\n",
    "    best_max_features = []\n",
    "    best_min_samples_leaf = []\n",
    "    best_max_depth = []\n",
    "    best_min_samples_split = []\n",
    "    \n",
    "    imputer = Imputer(missing_values = missing)    \n",
    "    rf_model = RandomForestClassifier(n_estimators = 200)    \n",
    "    \n",
    "    # For each fold, loads the training dataset in and perform the grid search on it.\n",
    "    for i in range(0, num_folds):\n",
    "        # Loads the dataset (training fold).\n",
    "        print(\"Loading training dataset...\") \n",
    "        dataset = genfromtxt(folder_name + '/' + 'fold' + str(i) + '_train.data', delimiter=\",\")\n",
    "        print(\"Training dataset was loaded in!\")\n",
    "\n",
    "        # Splits the dataset into the data and the labels.\n",
    "        print(\"Splitting the dataset into data and labels...\")\n",
    "        X = dataset[:, 0 : len(dataset[0]) - 1]\n",
    "        Y = dataset[:, len(dataset[0]) - 1]\n",
    "        X = imputer.fit_transform(X, Y)\n",
    "        print(\"The data and labels from the dataset have been split!\")\n",
    "\n",
    "        # Timed grid search.\n",
    "        start = timer()\n",
    "        param_grid = dict(max_features = max_features, min_samples_leaf = min_samples_leaf, \n",
    "                          max_depth = max_depth, min_samples_split = min_samples_split)\n",
    "        print(\"Starting the grid search...\")\n",
    "        kfold = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = 7)\n",
    "        grid_search = GridSearchCV(rf_model, param_grid, scoring = \"neg_log_loss\", n_jobs = -1, cv = kfold, verbose = 1)\n",
    "        grid_result = grid_search.fit(X, Y)\n",
    "        print(\"The grid search is over!\")\n",
    "        end = timer()\n",
    "        time_grid_rf.append(end - start)        \n",
    "\n",
    "        # Summarize results \n",
    "        print(\"Best: %f using %s \\n\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "        best_max_features.append(grid_result.best_params_['max_features'])        \n",
    "        best_min_samples_leaf.append(grid_result.best_params_['min_samples_leaf'])\n",
    "        best_max_depth.append(grid_result.best_params_['max_depth'])\n",
    "        best_min_samples_split.append(grid_result.best_params_['min_samples_split'])\n",
    "                \n",
    "        print(\"%.2f %%\\n\" % ((i + 1) / num_folds * 100))\n",
    "        \n",
    "    print(\"Done with the grid search!\\n\")\n",
    "    \n",
    "    # Grid extension.\n",
    "    parameters_rf = grid_extension_rf(folder_name, num_folds, missing, best_max_features, best_min_samples_leaf,\n",
    "                                                best_max_depth, best_min_samples_split)\n",
    "    \n",
    "    return parameters_rf\n",
    "    \n",
    "    \n",
    "# Grid search for Gradient Boosting. Checks also if the extension of the grid search\n",
    "# is necessary and if it is, performs it by calling the extended grid search function.\n",
    "def grid_search_gb(folder_name, num_folds, missing):\n",
    "    from numpy import genfromtxt\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    \n",
    "    # Initializes the parameters' list.\n",
    "    print(\"Initializing the parameters to test for Gradient Boosting...\") \n",
    "    learning_rate = [0.05, 0.1, 0.2]\n",
    "    max_depth = [3, 5, 6, 8]\n",
    "    subsample = [0.5, 0.8, 1]\n",
    "    max_features = [\"auto\", \"log2\", None]\n",
    "    min_samples_split = [2, 5, 8, 10]\n",
    "    print(\"The parameters to test for Gradient Boosting have been initialized!\\n\")\n",
    "    \n",
    "    # Initialize the arrays that'll contain the best parameters for each fold (10). Will be returned at the end.\n",
    "    best_learning_rate = []\n",
    "    best_max_depth = []\n",
    "    best_subsample = []\n",
    "    best_max_features = []\n",
    "    best_min_samples_split = []\n",
    "    \n",
    "    imputer = Imputer(missing_values = missing)    \n",
    "    gb_model = GradientBoostingClassifier(n_estimators = 200)    \n",
    "    \n",
    "    # For each fold, loads the training dataset in and perform the grid search on it.\n",
    "    for i in range(0, num_folds):\n",
    "        # Loads the dataset (training fold).\n",
    "        print(\"Loading training dataset...\") \n",
    "        dataset = genfromtxt(folder_name + '/' + 'fold' + str(i) + '_train.data', delimiter=\",\")\n",
    "        print(\"Training dataset was loaded in!\")\n",
    "\n",
    "        # Splits the dataset into the data and the labels.\n",
    "        print(\"Splitting the dataset into data and labels...\")\n",
    "        X = dataset[:, 0 : len(dataset[0]) - 1]\n",
    "        Y = dataset[:, len(dataset[0]) - 1]\n",
    "        X = imputer.fit_transform(X, Y)\n",
    "        print(\"The data and labels from the dataset have been split!\")\n",
    "\n",
    "        # Timed grid search.\n",
    "        start = timer()\n",
    "        param_grid = dict(learning_rate = learning_rate, max_depth = max_depth, subsample = subsample, \n",
    "                          max_features = max_features, min_samples_split = min_samples_split)\n",
    "        print(\"Starting the grid search...\")\n",
    "        kfold = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = 7)\n",
    "        grid_search = GridSearchCV(gb_model, param_grid, scoring = \"neg_log_loss\", n_jobs = -1, cv = kfold, verbose = 1)\n",
    "        grid_result = grid_search.fit(X, Y)\n",
    "        print(\"The grid search is over!\")\n",
    "        end = timer()\n",
    "        time_grid_gb.append(end - start)        \n",
    "\n",
    "        # Summarize results \n",
    "        print(\"Best: %f using %s \\n\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "        best_learning_rate.append(grid_result.best_params_['learning_rate'])\n",
    "        best_max_depth.append(grid_result.best_params_['max_depth'])\n",
    "        best_subsample.append(grid_result.best_params_['subsample'])\n",
    "        best_max_features.append(grid_result.best_params_['max_features'])  \n",
    "        best_min_samples_split.append(grid_result.best_params_['min_samples_split'])\n",
    "                \n",
    "        print(\"%.2f %%\\n\" % ((i + 1) / num_folds * 100))\n",
    "        \n",
    "    print(\"Done with the grid search!\\n\")\n",
    "    \n",
    "    # Grid extension.\n",
    "    parameters_gb = grid_extension_gb(folder_name, num_folds, missing, best_learning_rate, best_max_depth,\n",
    "                                      best_subsample, best_max_features, best_min_samples_split)\n",
    "    \n",
    "    return parameters_gb\n",
    "    \n",
    "    \n",
    "# Checks whether the grid extension is needed for XGBoost and performs it if it's the case.\n",
    "# In any case, also groups all the parameters into one 2D array to avoid having too\n",
    "# many parameters going around.\n",
    "def grid_extension_xgboost(folder_name, num_folds, missing, best_learning_rate, \n",
    "                             best_max_depth, best_subsample, best_gamma, \n",
    "                             best_min_child_weight):    \n",
    "    from numpy import genfromtxt\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    \n",
    "    need = False # Initializing the boolean that'll be use to know if the extension is needed. \n",
    "    \n",
    "    # Checks whether the grid extension is needed.\n",
    "    if (best_learning_rate.count(0.05) >= num_folds / 2 or best_learning_rate.count(0.2) >= num_folds / 2 \n",
    "        or best_max_depth.count(3) >= num_folds / 2 or best_max_depth.count(8) >= num_folds / 2 \n",
    "        or best_subsample.count(0.5) >= num_folds / 2 or best_subsample.count(1) >= num_folds / 2 \n",
    "        or best_gamma.count(0) >= num_folds / 2 or best_gamma.count(0.3) >= num_folds / 2\n",
    "        or best_min_child_weight.count(1) >= num_folds / 2 or best_min_child_weight.count(5) >= num_folds / 2):    \n",
    "        need = True\n",
    "   \n",
    "    # If the extension is needed...\n",
    "    if need == True:\n",
    "        print(\"\\nStarting the grid expansion...\")   \n",
    "\n",
    "        imputer = Imputer(missing_values = missing)\n",
    "\n",
    "        # For each fold, load the training dataset in and perform the grid search on it.\n",
    "        for i in range(0, num_folds):\n",
    "            # Load the dataset (training fold)\n",
    "            print(\"Loading training dataset...\") \n",
    "            dataset = genfromtxt(folder_name + '/' + 'fold' + str(i) + '_train.data', delimiter=\",\")\n",
    "            print(\"Training dataset was loaded in!\")\n",
    "\n",
    "            # Split the dataset into the data and the labels\n",
    "            print(\"Splitting the dataset into data and labels...\")\n",
    "            X = dataset[:, 0 : len(dataset[0]) - 1]\n",
    "            Y = dataset[:, len(dataset[0]) - 1]\n",
    "            X = imputer.fit_transform(X, Y)\n",
    "            print(\"The data and labels from the dataset have been split!\")\n",
    "\n",
    "            # Initializes the model with the best found parameter for each fold (individually).\n",
    "            # The parameters on the extreme of the grid will be added to the parameters' dictionary.\n",
    "            xgb_model = XGBClassifier(n_estimators = 200, learning_rate = best_learning_rate[i], max_depth = best_max_depth[i],\n",
    "                                     subsample = best_subsample[i], gamma = best_gamma[i], \n",
    "                                      min_child_weight = best_min_child_weight[i])\n",
    "\n",
    "            param_grid = dict()\n",
    "\n",
    "            if best_learning_rate[i] == 0.05 or best_learning_rate[i] == 0.2:\n",
    "                if best_learning_rate[i] == 0.05:\n",
    "                    learning_rate = [0.01, 0.03, 0.05, 0.07]\n",
    "                else:\n",
    "                    learning_rate = [0.15, 0.2, 0.25, 0.3]\n",
    "                param_grid['learning_rate'] = learning_rate\n",
    "                bool_lr = True\n",
    "            else:\n",
    "                bool_lr = False\n",
    "\n",
    "            if best_max_depth[i] == 3 or best_max_depth[i] == 8:\n",
    "                if best_max_depth[i] == 3:\n",
    "                    max_depth = [1, 2, 3, 4]\n",
    "                else:\n",
    "                    max_depth = [7, 8, 9, 10]\n",
    "                param_grid['max_depth'] = max_depth\n",
    "                bool_md = True\n",
    "            else:\n",
    "                bool_md = False\n",
    "\n",
    "            if best_subsample[i] == 0.5 or best_subsample[i] == 1:\n",
    "                if best_subsample[i] == 0.5:\n",
    "                    subsample = [0.4, 0.5, 0.6]\n",
    "                else:\n",
    "                    subsample = [0.9, 0.95, 1]\n",
    "                param_grid['subsample'] = subsample\n",
    "                bool_s = True\n",
    "            else:\n",
    "                bool_s = False\n",
    "\n",
    "            if best_gamma[i] == 0 or best_gamma[i] == 0.3:\n",
    "                if best_gamma[i] == 0:\n",
    "                    gamma = [0, 0.03, 0.05]\n",
    "                else:\n",
    "                    gamma = [0.25, 0.3, 0.4]\n",
    "                param_grid['gamma'] = gamma\n",
    "                bool_g = True\n",
    "            else:\n",
    "                bool_g = False\n",
    "\n",
    "            if best_min_child_weight[i] == 1 or best_min_child_weight[i] == 5:\n",
    "                if best_min_child_weight[i] == 1:\n",
    "                    min_child_weight = [0, 1, 2]\n",
    "                else:\n",
    "                    min_child_weight = [5, 6, 7, 8] \n",
    "                param_grid['min_child_weight'] = min_child_weight\n",
    "                bool_mcw = True\n",
    "            else:\n",
    "                bool_mcw = False\n",
    "\n",
    "            # Timed extended grid search.\n",
    "            start = timer()\n",
    "            print(\"Starting the (expended) grid search...\")\n",
    "            kfold = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = 7)\n",
    "            grid_search = GridSearchCV(xgb_model, param_grid, scoring = \"neg_log_loss\", n_jobs = -1, cv = kfold, verbose = 1)\n",
    "            grid_result = grid_search.fit(X, Y)\n",
    "            print(\"The (expended) grid search is over!\")\n",
    "            end = timer()\n",
    "            time_grid_xgb[i] += (end - start)\n",
    "\n",
    "            # Summarizes results.\n",
    "            print(\"Best: %f using %s \\n\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "            # Replaces the old parameters on the extreme of the grid by the newly found values.\n",
    "            if bool_lr == True:\n",
    "                best_learning_rate[i] = grid_result.best_params_['learning_rate']\n",
    "            if bool_md == True:\n",
    "                best_max_depth[i] = grid_result.best_params_['max_depth']\n",
    "            if bool_s == True:\n",
    "                best_subsample[i] = grid_result.best_params_['subsample']\n",
    "            if bool_g == True:\n",
    "                best_gamma[i] = grid_result.best_params_['gamma']\n",
    "            if bool_mcw == True:\n",
    "                best_min_child_weight[i] = grid_result.best_params_['min_child_weight']\n",
    "\n",
    "            print(\"%.2f %%\\n\" % ((i + 1) / num_folds * 100))\n",
    "\n",
    "        print(\"Done with the grid expansion!\\n\")  \n",
    "        \n",
    "    parameters_xgboost = [best_learning_rate, best_max_depth, best_subsample, best_gamma, best_min_child_weight]\n",
    "    \n",
    "    return parameters_xgboost \n",
    "\n",
    "\n",
    "# Checks whether the grid extension is needed for Random Forest and performs it if it's the case.\n",
    "# In any case, also groups all the parameters into one 2D array to avoid having too\n",
    "# many parameters going around.\n",
    "def grid_extension_rf(folder_name, num_folds, missing, best_max_features, best_min_samples_leaf,\n",
    "                                                best_max_depth, best_min_samples_split):    \n",
    "    from numpy import genfromtxt\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    \n",
    "    need = False # Initializing the boolean that'll be use to know if the extension is needed. \n",
    "    \n",
    "    # Checks whether the grid extension is needed.\n",
    "    if (best_min_samples_leaf.count(1) >= num_folds / 2 or best_min_samples_leaf.count(70) >= num_folds / 2 \n",
    "        or best_max_depth.count(10) >= num_folds / 2 or best_min_samples_split.count(1) >= num_folds / 2 \n",
    "        or best_min_samples_split.count(10) >= num_folds / 2):\n",
    "        need = True\n",
    "        \n",
    "    # If the extension is needed...\n",
    "    if need == True:\n",
    "        print(\"\\nStarting the grid expansion...\")   \n",
    "\n",
    "        imputer = Imputer(missing_values = missing)\n",
    "        # For each fold, loads the training dataset in and perform the grid search on it.\n",
    "        for i in range(0, num_folds):\n",
    "            # Loads the dataset (training fold).\n",
    "            print(\"Loading training dataset...\") \n",
    "            dataset = genfromtxt(folder_name + '/' + 'fold' + str(i) + '_train.data', delimiter=\",\")\n",
    "            print(\"Training dataset was loaded in!\")\n",
    "\n",
    "            # Splits the dataset into the data and the labels.\n",
    "            print(\"Splitting the dataset into data and labels...\")\n",
    "            X = dataset[:, 0 : len(dataset[0]) - 1]\n",
    "            Y = dataset[:, len(dataset[0]) - 1]\n",
    "            X = imputer.fit_transform(X, Y)\n",
    "            print(\"The data and labels from the dataset have been split!\")\n",
    "\n",
    "            # Initializes the model with the best found parameter for each fold (individually).\n",
    "            # The parameters on the extreme of the grid will be added to the parameters' dictionary.\n",
    "            rf_model = RandomForestClassifier(n_estimators = 200, max_features = best_max_features[i],\n",
    "                                              min_samples_leaf = best_min_samples_leaf[i], max_depth = best_max_depth[i],\n",
    "                                             min_samples_split = best_min_samples_split[i])\n",
    "    \n",
    "            param_grid = dict()\n",
    "        \n",
    "            if best_min_samples_leaf[i] == 1 or best_min_samples_leaf[i] == 70:\n",
    "                if best_min_samples_leaf[i] == 1:\n",
    "                    min_samples_leaf = [1, 5, 10, 15]\n",
    "                else:\n",
    "                    min_samples_leaf = [60, 70, 80]\n",
    "                param_grid['min_samples_leaf'] = min_samples_leaf\n",
    "                bool_msl = True\n",
    "            else:\n",
    "                bool_msl = False\n",
    "            \n",
    "            if best_max_depth[i] == 10:\n",
    "                max_depth = [9, 10, 15, 20]\n",
    "                param_grid['max_depth'] = max_depth\n",
    "                bool_md = True\n",
    "            else:\n",
    "                bool_md = False\n",
    "                \n",
    "            if best_min_samples_split[i] == 1 or best_min_samples_split[i] == 10:\n",
    "                if best_min_samples_split[i] == 1:\n",
    "                    min_samples_split = [1, 2, 3, 4]\n",
    "                else:\n",
    "                    min_samples_split = [9, 10, 11, 15]\n",
    "                param_grid['min_samples_split'] = min_samples_split\n",
    "                bool_mss = True\n",
    "            else:\n",
    "                bool_mss = False\n",
    "\n",
    "            # Timed extended grid search.\n",
    "            start = timer()\n",
    "            print(\"Starting the (expended) grid search...\")\n",
    "            kfold = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = 7)\n",
    "            grid_search = GridSearchCV(rf_model, param_grid, scoring = \"neg_log_loss\", n_jobs = -1, cv = kfold, verbose = 1)\n",
    "            grid_result = grid_search.fit(X, Y)\n",
    "            print(\"The (expended) grid search is over!\")\n",
    "            end = timer()\n",
    "            time_grid_rf[i] += (end - start)\n",
    "\n",
    "            # Summarizes results.\n",
    "            print(\"Best: %f using %s \\n\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "            # Replaces the old parameters on the extreme of the grid by the newly found values.\n",
    "            if bool_msl == True:\n",
    "                best_min_samples_leaf[i] = grid_result.best_params_['min_samples_leaf']            \n",
    "            if bool_md == True:\n",
    "                best_max_depth[i] = grid_result.best_params_['max_depth']\n",
    "            if bool_mss == True:\n",
    "                best_min_samples_split[i] = grid_result.best_params_['min_samples_split']\n",
    "\n",
    "            print(\"%.2f %%\\n\" % ((i + 1) / num_folds * 100))\n",
    "\n",
    "        print(\"Done with the grid expansion!\\n\")  \n",
    "        \n",
    "    parameters_rf = [best_max_features, best_min_samples_leaf, best_max_depth, best_min_samples_split] \n",
    "    \n",
    "    return parameters_rf\n",
    "\n",
    "\n",
    "\n",
    "# Checks whether the grid extension is needed for XGBoost and performs it if it's the case.\n",
    "# In any case, also groups all the parameters into one 2D array to avoid having too\n",
    "# many parameters going around.\n",
    "def grid_extension_gb(folder_name, num_folds, missing, best_learning_rate, \n",
    "                             best_max_depth, best_subsample, best_max_features, \n",
    "                             best_min_samples_split):    \n",
    "    from numpy import genfromtxt\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    \n",
    "    need = False # Initializing the boolean that'll be use to know if the extension is needed. \n",
    "    \n",
    "    # Checks whether the grid extension is needed.\n",
    "    if (best_learning_rate.count(0.05) >= num_folds / 2 or best_learning_rate.count(0.2) >= num_folds / 2 \n",
    "        or best_max_depth.count(3) >= num_folds / 2 or best_max_depth.count(8) >= num_folds / 2 \n",
    "        or best_subsample.count(0.5) >= num_folds / 2 or best_subsample.count(1) >= num_folds / 2 \n",
    "        or best_min_samples_split.count(1) >= num_folds / 2 or best_min_samples_split.count(10) >= num_folds / 2):    \n",
    "        need = True\n",
    "   \n",
    "    # If the extension is needed...\n",
    "    if need == True:\n",
    "        print(\"\\nStarting the grid expansion...\")   \n",
    "\n",
    "        imputer = Imputer(missing_values = missing)\n",
    "\n",
    "        # For each fold, load the training dataset in and perform the grid search on it.\n",
    "        for i in range(0, num_folds):\n",
    "            # Load the dataset (training fold)\n",
    "            print(\"Loading training dataset...\") \n",
    "            dataset = genfromtxt(folder_name + '/' + 'fold' + str(i) + '_train.data', delimiter=\",\")\n",
    "            print(\"Training dataset was loaded in!\")\n",
    "\n",
    "            # Split the dataset into the data and the labels\n",
    "            print(\"Splitting the dataset into data and labels...\")\n",
    "            X = dataset[:, 0 : len(dataset[0]) - 1]\n",
    "            Y = dataset[:, len(dataset[0]) - 1]\n",
    "            X = imputer.fit_transform(X, Y)\n",
    "            print(\"The data and labels from the dataset have been split!\")\n",
    "\n",
    "            # Initializes the model with the best found parameter for each fold (individually).\n",
    "            # The parameters on the extreme of the grid will be added to the parameters' dictionary.\n",
    "            gb_model = GradientBoostingClassifier(n_estimators = 200, learning_rate = best_learning_rate[i], \n",
    "                                                  max_depth = best_max_depth[i], subsample = best_subsample[i], \n",
    "                                                  max_features = best_max_features[i], min_samples_split = best_min_samples_split[i])\n",
    "\n",
    "            param_grid = dict()\n",
    "\n",
    "            if best_learning_rate[i] == 0.05 or best_learning_rate[i] == 0.2:\n",
    "                if best_learning_rate[i] == 0.05:\n",
    "                    learning_rate = [0.01, 0.03, 0.05, 0.07]\n",
    "                else:\n",
    "                    learning_rate = [0.15, 0.2, 0.25, 0.3]\n",
    "                param_grid['learning_rate'] = learning_rate\n",
    "                bool_lr = True\n",
    "            else:\n",
    "                bool_lr = False\n",
    "\n",
    "            if best_max_depth[i] == 3 or best_max_depth[i] == 8:\n",
    "                if best_max_depth[i] == 3:\n",
    "                    max_depth = [1, 2, 3, 4]\n",
    "                else:\n",
    "                    max_depth = [7, 8, 9, 10]\n",
    "                param_grid['max_depth'] = max_depth\n",
    "                bool_md = True\n",
    "            else:\n",
    "                bool_md = False\n",
    "\n",
    "            if best_subsample[i] == 0.5 or best_subsample[i] == 1:\n",
    "                if best_subsample[i] == 0.5:\n",
    "                    subsample = [0.4, 0.5, 0.6]\n",
    "                else:\n",
    "                    subsample = [0.9, 0.95, 1]\n",
    "                param_grid['subsample'] = subsample\n",
    "                bool_s = True\n",
    "            else:\n",
    "                bool_s = False\n",
    "\n",
    "            if best_min_samples_split[i] == 1 or best_min_samples_split[i] == 10:\n",
    "                if best_min_samples_split[i] == 1:\n",
    "                    min_samples_split = [1, 2, 3, 4]\n",
    "                else:\n",
    "                    min_samples_split = [9, 10, 11, 15]\n",
    "                param_grid['min_samples_split'] = min_samples_split\n",
    "                bool_mss = True\n",
    "            else:\n",
    "                bool_mss = False\n",
    "\n",
    "            # Timed extended grid search.\n",
    "            start = timer()\n",
    "            print(\"Starting the (expended) grid search...\")\n",
    "            kfold = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = 7)\n",
    "            grid_search = GridSearchCV(gb_model, param_grid, scoring = \"neg_log_loss\", n_jobs = -1, cv = kfold, verbose = 1)\n",
    "            grid_result = grid_search.fit(X, Y)\n",
    "            print(\"The (expended) grid search is over!\")\n",
    "            end = timer()\n",
    "            time_grid_gb[i] += (end - start)\n",
    "\n",
    "            # Summarizes results.\n",
    "            print(\"Best: %f using %s \\n\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "            # Replaces the old parameters on the extreme of the grid by the newly found values.\n",
    "            if bool_lr == True:\n",
    "                best_learning_rate[i] = grid_result.best_params_['learning_rate']\n",
    "            if bool_md == True:\n",
    "                best_max_depth[i] = grid_result.best_params_['max_depth']\n",
    "            if bool_s == True:\n",
    "                best_subsample[i] = grid_result.best_params_['subsample']\n",
    "            if bool_mss == True:\n",
    "                best_min_samples_split[i] = grid_result.best_params_['min_samples_split']\n",
    "\n",
    "            print(\"%.2f %%\\n\" % ((i + 1) / num_folds * 100))\n",
    "\n",
    "        print(\"Done with the grid expansion!\\n\")  \n",
    "        \n",
    "    parameters_gb = [best_learning_rate, best_max_depth, best_subsample, best_max_features, best_min_samples_split]\n",
    "    \n",
    "    return parameters_gb \n",
    "\n",
    "\n",
    "\n",
    "# Trains all the models with the final parameters and tests them.\n",
    "def train_and_test(folder_name, nums_folds, missing, parameters_xgboost, parameters_rf, parameters_gb):\n",
    "    from numpy import genfromtxt\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    \n",
    "    # Initializes the default models.\n",
    "    print(\"Initializing all the default models...\")\n",
    "    xgb_d_model = XGBClassifier(n_estimators = 200)\n",
    "    rf_d_model = RandomForestClassifier(n_estimators = 200)\n",
    "    gb_d_model = GradientBoostingClassifier(n_estimators = 200)\n",
    "    print(\"All the default models have been initialized!\")\n",
    "    \n",
    "    # Will store the accuracy for each fold. An average will then be computed.\n",
    "    xgb_d_results = []\n",
    "    xgb_results = []\n",
    "    rf_d_results = []\n",
    "    rf_results = []\n",
    "    gb_d_results = []\n",
    "    gb_results = []\n",
    "    \n",
    "    imputer = Imputer(missing_values = missing)\n",
    "    \n",
    "    depth_rf_d = []\n",
    "    depth_rf = []\n",
    "    \n",
    "    # For each fold, loads the training/testing dataset in and fits the models before testing.\n",
    "    for i in range(0, num_folds):\n",
    "        # Loads the dataset (training fold).\n",
    "        print(\"Loading the training set...\")\n",
    "        dataset = genfromtxt(folder_name + '/' + 'fold' + str(i) + '_train.data', delimiter=\",\")\n",
    "        print(\"Training set was loaded in!\")\n",
    "        \n",
    "        # Split the dataset into the data and the labels\n",
    "        print(\"Splitting the dataset into data and labels...\")\n",
    "        X = dataset[:, 0 : len(dataset[0]) - 1]\n",
    "        Y = dataset[:, len(dataset[0]) - 1]        \n",
    "        X = imputer.fit_transform(X, Y)\n",
    "        print(\"The data and labels from the dataset have been split!\")\n",
    "\n",
    "        print(\"Training all the default models over the fold with the best parameters...\")\n",
    "        start = timer()\n",
    "        xgb_d_model.fit(X, Y)\n",
    "        end = timer()\n",
    "        time_fit_xgb_d.append(end - start) \n",
    "        \n",
    "        start = timer()\n",
    "        rf_model.fit(X, Y)\n",
    "        end = timer()\n",
    "        time_fit_rf.append(end - start)\n",
    "        \n",
    "        start = timer()\n",
    "        gb_model.fit(X, Y)\n",
    "        end = timer()\n",
    "        time_fit_gb.append(end - start)\n",
    "        print(\"All the default models have been trained!\")\n",
    "        \n",
    "        print(\"Initializing all the tuned models and training them over the fold with the best parameters...\")\n",
    "        xgb_model = XGBClassifier(n_estimators = 200, learning_rate = parameters_xgboost[0][i], \n",
    "                                  max_depth = parameters_xgboost[1][i], subsample = parameters_xgboost[2][i],\n",
    "                                  gamma = parameters_xgboost[3][i], min_child_weight = parameters_xgboost[4][i])\n",
    "        \n",
    "        rf_model = RandomForestClassifier(n_estimators = 200, max_features = parameters_rf[0][i], \n",
    "                                          min_samples_leaf = parameters_rf[1][i], max_depth = parameters_rf[2][i],\n",
    "                                          min_samples_split = parameters_rf[3][i])\n",
    "        \n",
    "        gb_model = GradientBoostingClassifier(n_estimators = 200, learning_rate = parameters_gb[0][i], \n",
    "                                              max_depth = parameters_gb[1][i], subsample = parameters_gb[2][i],\n",
    "                                              max_features = parameters_gb[3][i], min_samples_split = parameters_gb[4][i])\n",
    "        \n",
    "        start = timer()\n",
    "        xgb_model.fit(X, Y)\n",
    "        end = timer()\n",
    "        time_fit_xgb.append(end - start)\n",
    "        \n",
    "        start = timer()\n",
    "        rf_model.fit(X, Y)\n",
    "        end = timer()\n",
    "        time_fit_rf.append(end - start)\n",
    "        \n",
    "        start = timer()\n",
    "        gb_model.fit(X, Y)\n",
    "        end = timer()\n",
    "        time_fit_gb.append(end - start)\n",
    "        print(\"All the tuned models have been initialized and trained!\")\n",
    "        \n",
    "        # Load the dataset (testing fold)\n",
    "        print(\"Loading testing dataset...\") \n",
    "        testing = genfromtxt(folder_name + '/' + 'fold' + str(i) + '_test.data', delimiter=\",\")\n",
    "        print(\"Testing dataset was loaded in!\")\n",
    "\n",
    "        # Split the testing into the data and the labels\n",
    "        print(\"Splitting the testing set into data and labels...\")\n",
    "        X_test = testing[:, 0 : len(dataset[0]) - 1]\n",
    "        Y_test = testing[:, len(dataset[0]) - 1]\n",
    "        X_test = imputer.fit_transform(X_test, Y_test);\n",
    "        print(\"The data and labels from the testing set have been split!\\n\")\n",
    "        \n",
    "        # Make predictions for test data\n",
    "        xgb_d_Y_pred = xgb_d_model.predict(X_test)\n",
    "        xgb_d_predictions = [round(value) for value in xgb_d_Y_pred]\n",
    "        xgb_Y_pred = xgb_model.predict(X_test)\n",
    "        xgb_predictions = [round(value) for value in xgb_Y_pred]\n",
    "        rf_d_Y_pred = rf_d_model.predict(X_test)\n",
    "        rf_d_predictions = [round(value) for value in rf_d_Y_pred]\n",
    "        rf_Y_pred = rf_model.predict(X_test)\n",
    "        rf_predictions = [round(value) for value in rf_Y_pred]\n",
    "        gb_d_Y_pred = gb_d_model.predict(X_test)\n",
    "        gb_d_predictions = [round(value) for value in gb_d_Y_pred]\n",
    "        gb_Y_pred = gb_model.predict(X_test)\n",
    "        gb_predictions = [round(value) for value in gb_Y_pred]\n",
    "\n",
    "        # Evaluate predictions\n",
    "        xgb_d_accuracy = accuracy_score(Y_test, xgb_d_predictions)\n",
    "        xgb_accuracy = accuracy_score(Y_test, xgb_predictions)\n",
    "        rf_d_accuracy = accuracy_score(Y_test, rf_d_predictions)\n",
    "        rf_accuracy = accuracy_score(Y_test, rf_predictions)\n",
    "        gb_d_accuracy = accuracy_score(Y_test, gb_d_predictions)\n",
    "        gb_accuracy = accuracy_score(Y_test, gb_predictions)\n",
    "\n",
    "        # Save the predictions results\n",
    "        xgb_d_results.append(xgb_d_accuracy)\n",
    "        xgb_results.append(xgb_accuracy)\n",
    "        rf_d_results.append(rf_d_accuracy)\n",
    "        rf_results.append(rf_accuracy)\n",
    "        gb_d_results.append(gb_d_accuracy)\n",
    "        gb_results.append(gb_accuracy)\n",
    "        \n",
    "        print(\"Default XGBoost accuracy %.2f%%\" % (xgb_d_accuracy * 100.0))\n",
    "        print(\"XGBoost accuracy: %.2f%%\" % (xgb_accuracy * 100.0))\n",
    "        print(\"Default Random Forests accuracy: %.2f%%\" % (rf_d_accuracy * 100.0))\n",
    "        print(\"Random Forests accuracy: %.2f%%\" % (rf_accuracy * 100.0))\n",
    "        print(\"Default Gradient Boosting accuracy: %.2f%%\" % (gb_d_accuracy * 100.0))\n",
    "        print(\"Gradient Boosting accuracy: %.2f%% \\n\" % (gb_accuracy * 100.0))\n",
    "\n",
    "        print(\"%.2f %%\\n\" % ((i + 1) / num_folds * 100))\n",
    "        \n",
    "        # Saving the depth of the trees for random forests.  \n",
    "        tmp = [estimator.tree_.max_depth for estimator in rf_d_model.estimators_]\n",
    "        depth_rf_d.append(sum(tmp) / len(tmp)) \n",
    "        tmp = [estimator.tree_.max_depth for estimator in rf_model.estimators_]\n",
    "        depth_rf.append(sum(tmp) / len(tmp))\n",
    "     \n",
    "    return xgb_d_results, xgb_results, rf_d_results, rf_results, gb_d_results, gb_results, depth_rf_d, depth_rf\n",
    "    \n",
    "    \n",
    "    \n",
    "# Compare the three classifiers.    \n",
    "def compare_classifiers(folder_name, dataset_name, num_folds, missing):\n",
    "    # Initializing all the variables for the time measurements.\n",
    "    global time_grid_xgb, time_grid_rf, time_grid_gb\n",
    "    global time_fit_xgb_d, time_fit_xgb\n",
    "    global time_fit_rf_d, time_fit_rf\n",
    "    global time_fit_gb_d, time_fit_gb\n",
    "    time_grid_xgb = []\n",
    "    time_grid_rf = []\n",
    "    time_grid_gb = []\n",
    "    time_fit_xgb_d = []\n",
    "    time_fit_xgb = []\n",
    "    time_fit_rf_d = []\n",
    "    time_fit_rf = []\n",
    "    time_fit_gb_d = []\n",
    "    time_fit_gb = []\n",
    "    \n",
    "    # Splitting the dataset\n",
    "    splitting_dataset(folder_name, dataset_name, num_folds)    \n",
    "    # Learning the parameters over the basic grid for XGBoost, RF and GB\n",
    "    parameters_xgboost, parameters_rf, parameters_gb = general_grid_search(folder_name, num_folds, missing)    \n",
    "    # Training and testing with the best parameters of each fold for each classifier.\n",
    "    train_and_test(folder_name, nums_folds, missing, parameters_xgboost, parameters_rf, parameters_gb)\n",
    "    # Printing stuff (accuracy, most frequent parameters, depth, std).\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "compare_classifiers('Datasets', 'iris.data', 3, \"NaN\")\n",
    "print(\"ok\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
